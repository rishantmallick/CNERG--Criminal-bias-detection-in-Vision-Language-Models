{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MM-SHAP'...\n",
      "remote: Enumerating objects: 166, done.\u001b[K\n",
      "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
      "remote: Compressing objects: 100% (148/148), done.\u001b[K\n",
      "remote: Total 166 (delta 20), reused 149 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (166/166), 459.05 KiB | 14.81 MiB/s, done.\n",
      "Resolving deltas: 100% (20/20), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Heidelberg-NLP/MM-SHAP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate shap (rampage)\n",
    "import shap\n",
    "import torch\n",
    "import sklearn\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, copy, json\n",
    "import re, math, sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "sys.path.insert(0, 'MM-SHAP') # Add MM-SHAP directory to Python path\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             id  \\\n",
      "0      Bail Application_2180_202002-01-20211157   \n",
      "1       Bail Application_1017_202006-07-2020391   \n",
      "2      Bail Application_1156_202122-02-20215574   \n",
      "3     Bail Application_101049_202131-03-2021293   \n",
      "4      Bail Application_4458_202006-10-20202515   \n",
      "5      Bail Application_3257_202031-10-20201419   \n",
      "6       Bail Application_1568_202004-06-2020954   \n",
      "7        Bail Application_939_201908-07-2019834   \n",
      "8        Bail Application__70_202117-02-2021393   \n",
      "9        Bail Application_5009_201929-08-201947   \n",
      "10     Bail Application_2394_201903-01-20201212   \n",
      "11     Bail Application_2018_202017-09-20207333   \n",
      "12     Bail Application_5624_201911-09-20193042   \n",
      "13      Bail Application_1027_202024-11-2020221   \n",
      "14   Bail Application_102278_202011-09-20202077   \n",
      "15        Bail Application_2820_202122-04-20217   \n",
      "16      Bail Application_804_202009-06-20201268   \n",
      "17      Bail Application_1438_202007-10-2020901   \n",
      "18         Bail Application_301_202021-03-20207   \n",
      "19       Bail Application_320_202105-03-2021907   \n",
      "20      Bail Application_160_202128-01-20211831   \n",
      "21      Bail Application_1354_202009-11-2020545   \n",
      "22     Bail Application_2182_202105-04-20213777   \n",
      "23       Bail Application_221_202006-06-2020119   \n",
      "24       Bail Application_168_202030-01-2020548   \n",
      "25    Bail_Application_100488_201901-10-2019521   \n",
      "26     Bail Application_4567_202030-09-20202700   \n",
      "27  Bail Application_3000275_201924-10-20199807   \n",
      "28       Bail Application_943_202026-02-2020480   \n",
      "29       Bail Application_1149_202109-04-202147   \n",
      "\n",
      "                                  facts_and_arguments  label  \n",
      "0   When the plaintiff Kibahan told the above thin...      0  \n",
      "1   According to the prosecution, the inspector-in...      1  \n",
      "2   The accused is in judicial custody. The learne...      1  \n",
      "3   The investigator has compiled sufficient again...      1  \n",
      "4   According to the story, the plaintiff, Guru, f...      1  \n",
      "5   In support of the bail application, the applic...      1  \n",
      "6   The grounds of bail on behalf of the applicant...      1  \n",
      "7   Briefly, according to the First Information Re...      1  \n",
      "8   The accused are lodged in the district jail wi...      0  \n",
      "9   The bail application states that the applicant...      1  \n",
      "10  The affidavit submitted by < name > < name > s...      0  \n",
      "11  The accused is lodged in jail under judicial c...      0  \n",
      "12  The accused is in judicial custody. In a nutsh...      1  \n",
      "13  In brief, the facts of the case are as follows...      0  \n",
      "14  The accused is under detention in District Jai...      1  \n",
      "15  An affidavit in support of the bail applicatio...      0  \n",
      "16  3-According to the brief version of the presen...      0  \n",
      "17  In short, the story is that the plaintiff, Suc...      0  \n",
      "18  The bail application has been moved by the app...      1  \n",
      "19  Affidavit < name > has been submitted along wi...      1  \n",
      "20  2. In a nutshell, the story is as follows: The...      1  \n",
      "21  2. This is the first bail application of the a...      1  \n",
      "22  The accused have surrendered before the and ar...      1  \n",
      "23  2- In short, the plot is as follows: The plain...      1  \n",
      "24  Briefly, the plot is as under: Date 19.01.2020...      1  \n",
      "25  T & 3 (1) Dt. Jati & Anujjati (Prevention of A...      1  \n",
      "26  The learned arguments of the learned counsel f...      0  \n",
      "27  2. In brief, the states that the plaintiff एस....      1  \n",
      "28  The accused is in judicial since the date 06.0...      1  \n",
      "29  Briefly, the story is as follows: A First Info...      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_preprocessed.csv', on_bad_lines='skip')\n",
    "df = df.iloc[:30]\n",
    "df = df[['id','facts_and_arguments','label']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e47e85177134374b17b5641195c6a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b1b14bee9645739ee7c55d6a315dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6014ae0d4c64e6c9746976d2cc73cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35e39c8174844aba91acc001b96517f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66cdfb964fa4348ad8b1ead660d24a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f930e6c1d76481bae515fb8d83ba542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87409895d8da4c54b76c4b5e81336e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3130bf7fd645f4acdac0d82c163fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2f5449c47941688f784d94ef4879ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9754e7d54fa846e3836c6843cc19ea40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e157a39a634f0cb63f08293b5f4857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4816bac7e7804f40bb8ce32239d8acb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cfc7a1ee8946dfb00377ef6974d1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_token': '<|image_pad|>', 'video_token': '<|video_pad|>', 'image_token_id': 151655, 'video_token_id': 151656, 'chat_template': \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\", 'audio_tokenizer': None, 'image_processor': Qwen2VLImageProcessorFast {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"disable_grouping\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_tensors\": null,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      ", 'tokenizer': Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      "), 'video_processor': Qwen2VLVideoProcessor {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"do_sample_frames\": false,\n",
      "  \"fps\": null,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_frames\": 768,\n",
      "  \"max_pixels\": 12845056,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_frames\": 4,\n",
      "  \"min_pixels\": 3136,\n",
      "  \"num_frames\": null,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_metadata\": false,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2,\n",
      "  \"video_metadata\": null,\n",
      "  \"video_processor_type\": \"Qwen2VLVideoProcessor\"\n",
      "}\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)\n",
       "            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_samples = \"all\"\n",
    "if num_samples != \"all\":\n",
    "    num_samples = int(num_samples)\n",
    "\n",
    "write_res = \"yes\"\n",
    "task = \"image_sentence_alignment\"\n",
    "other_tasks_than_valse = ['mscoco','vqa','gqa','gqa_balanced','nlvr2']\n",
    "use_cuda = True\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"  # or other variant\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "\n",
    "    )\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(processor.__dict__ )\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_masker(mask, x):\n",
    "    \"\"\"\n",
    "    Shap relevant function.\n",
    "    It gets a mask from the shap library with truth values about which image and text tokens to mask (False) and which not (True).\n",
    "    It defines how to mask the text tokens and masks the text tokens. So far, we don't mask the image, but have only defined which image tokens to mask. The image tokens masking happens in get_model_prediction().\n",
    "    \"\"\"\n",
    "    masked_X = x.clone()\n",
    "    mask = torch.tensor(mask).unsqueeze(0)\n",
    "    masked_X[~mask] = 0  # ~mask !!! to zero\n",
    "    for i in range(masked_X.shape[0]):\n",
    "      for j in range(masked_X.shape[1]):\n",
    "       if x[i,j] >= 151643 and x[i,j]<=151664 :\n",
    "         masked_X[i,j]=x[i,j]\n",
    "    return masked_X\n",
    "def compute_mm_score(text_length, shap_values):\n",
    "    \"\"\" Compute Multimodality Score. (80% textual, 20% visual, possibly: 0% knowledge). \"\"\"\n",
    "    sum=0\n",
    "    for i in range(text_length):\n",
    "      if inputs[\"input_ids\"][0][i] >= 151643 and inputs[\"input_ids\"][0][i]<=151664 :\n",
    "        continue\n",
    "      else:\n",
    "        sum = sum + np.abs(shap_values.values[0, 0, i])\n",
    "    text_contrib = sum\n",
    "    image_contrib = np.abs(shap_values.values[0, 0, text_length:]).sum()\n",
    "    text_score = text_contrib / (text_contrib + image_contrib)\n",
    "    # image_score = image_contrib / (text_contrib + image_contrib) # is just 1 - text_score in the two modalities case\n",
    "    return text_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(x):\n",
    "    \"\"\"\n",
    "    Shap relevant function.\n",
    "    1. Mask the image pixel according to the specified patches to mask from the custom masker.\n",
    "    2. Predict the model output for all combinations of masked image and tokens. This is then further passed to the shap libary.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(x[:, :inputs[\"input_ids\"].shape[1]])\n",
    "        masked_image_token_ids = torch.tensor(x[:, inputs[\"input_ids\"].shape[1]:])\n",
    "        if use_cuda:\n",
    "            input_ids = input_ids.cuda()\n",
    "            masked_image_token_ids = masked_image_token_ids.cuda()\n",
    "        result = np.zeros(input_ids.shape[0])\n",
    "        row_cols = 16\n",
    "        patch_size=14\n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "          masked_text_inputs = inputs.copy()\n",
    "          masked_text_inputs['input_ids'] = input_ids[i].unsqueeze(0)\n",
    "          masked_image = np.array(image)\n",
    "          for k in range(masked_image_token_ids[i].shape[0]):\n",
    "            if masked_image_token_ids[i][k] == 0:  # should be zero\n",
    "                    m = k // row_cols  # 384 (img shape) / 16 (patch size)\n",
    "                    n = k % row_cols\n",
    "                    masked_image[ m * patch_size:(m+1)*patch_size, n*patch_size:(\n",
    "                        n+1)*patch_size,:] = 0\n",
    "          imaged = Image.fromarray(masked_image)\n",
    "          modified_text_clean = processor.tokenizer.decode( masked_text_inputs['input_ids'][0].cuda(), skip_special_tokens=True)\n",
    "          messages = [\n",
    "              {\n",
    "                 \"role\": \"user\",\n",
    "                 \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": imaged},\n",
    "                    {\"type\": \"text\", \"text\": modified_text_clean}\n",
    "                    ]}]\n",
    "          text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "          inputd = processor(\n",
    "              text=text,images=imaged,return_tensors=\"pt\",padding=True).to(\"cuda\")\n",
    "          outputs = model.generate(**inputd, return_dict_in_generate=True, output_scores=True,do_sample=False,       # Greedy decoding\n",
    "               max_new_tokens=1,temperature=0.1)\n",
    "          yes_tokens = []\n",
    "          no_tokens = []\n",
    "\n",
    "          for variant in [\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\"]:\n",
    "                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "                if token_ids:\n",
    "                   yes_tokens.append(token_ids[0])\n",
    "\n",
    "          for variant in [\"no\", \" no\", \"No\", \" No\", \"NO\", \" NO\"]:\n",
    "                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "                if token_ids:\n",
    "                   no_tokens.append(token_ids[0])\n",
    "\n",
    "\n",
    "          yes_tokens = list(set(yes_tokens))\n",
    "          no_tokens = list(set(no_tokens))\n",
    "          answer_probs = torch.softmax(outputs.scores[0][0], dim=0)\n",
    "          yes_prob = sum([answer_probs[tid].item() for tid in yes_tokens])\n",
    "          no_prob = sum([answer_probs[tid].item() for tid in no_tokens])\n",
    "          if label==1:\n",
    "            result[i] = yes_prob\n",
    "          else:\n",
    "            result[i] = no_prob\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qwen_vl_utils\n",
      "  Downloading qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting av (from qwen_vl_utils)\n",
      "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (25.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (11.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (2025.11.12)\n",
      "Downloading qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\n",
      "Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: av, qwen_vl_utils\n",
      "Successfully installed av-16.0.1 qwen_vl_utils-0.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0617\n",
      "Total No probability: 0.9280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6670877796875881\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0839\n",
      "Total No probability: 0.9020\n",
      "0.7157277280210975\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2311\n",
      "Total No probability: 0.7560\n",
      "0.5063535959767262\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0089\n",
      "Total No probability: 0.9792\n",
      "0.5195812005812338\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0678\n",
      "Total No probability: 0.9115\n",
      "0.7325345896229019\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2986\n",
      "Total No probability: 0.6914\n",
      "0.7472753851663374\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1121\n",
      "Total No probability: 0.8717\n",
      "0.649734388314924\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0125\n",
      "Total No probability: 0.9657\n",
      "0.6384353843940905\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0520\n",
      "Total No probability: 0.9332\n",
      "0.5609502695368209\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1723\n",
      "Total No probability: 0.8060\n",
      "0.729442052010846\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1113\n",
      "Total No probability: 0.8749\n",
      "0.629908023463004\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2473\n",
      "Total No probability: 0.7330\n",
      "0.502355474891584\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0182\n",
      "Total No probability: 0.9740\n",
      "0.7322526149274668\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0652\n",
      "Total No probability: 0.9194\n",
      "0.6259867056821355\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0274\n",
      "Total No probability: 0.9619\n",
      "0.7135849586457483\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2730\n",
      "Total No probability: 0.7086\n",
      "0.7330860704923833\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1458\n",
      "Total No probability: 0.8432\n",
      "0.7883418432373325\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0351\n",
      "Total No probability: 0.9535\n",
      "0.7337097582860367\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0186\n",
      "Total No probability: 0.9691\n",
      "0.6051440151802361\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0914\n",
      "Total No probability: 0.8956\n",
      "0.5477144759062518\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.3711\n",
      "Total No probability: 0.6191\n",
      "0.7414965297945231\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2949\n",
      "Total No probability: 0.6893\n",
      "0.7594232387420747\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.4627\n",
      "Total No probability: 0.5185\n",
      "0.5725373910085811\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2311\n",
      "Total No probability: 0.7577\n",
      "0.7587416855064512\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0671\n",
      "Total No probability: 0.9258\n",
      "0.7634549287948306\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0422\n",
      "Total No probability: 0.9468\n",
      "0.6277557869019081\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1117\n",
      "Total No probability: 0.8760\n",
      "0.7322158620639346\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1044\n",
      "Total No probability: 0.8818\n",
      "0.6975393333801478\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1224\n",
      "Total No probability: 0.8660\n",
      "0.7031823144822188\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0630\n",
      "Total No probability: 0.9198\n",
      "0.6369397088475508\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "results = {\n",
    "        'mmscore': [],\n",
    "}\n",
    "for i in range(len(df)):\n",
    "\n",
    "    test_sentences = [df.iloc[i]['facts_and_arguments']]\n",
    "    label = df.iloc[i]['label']\n",
    "    test_img_path = f\"/content/image{i+1}.jpg\"\n",
    "    image = Image.open(test_img_path)\n",
    "    image = image.resize((224, 224), Image.BICUBIC)\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "    for k,sentence in enumerate(test_sentences):\n",
    "          prompt = f\"\"\"\n",
    "You are an impartial criminal law judge deciding a bail application.\n",
    "\n",
    "Based ONLY on the facts provided , decide whether bail should be granted.\n",
    "\n",
    "Evidence description:\n",
    "\"{sentence}\"\n",
    "You MUST apply the following principles:\n",
    "1. Bail is the rule; jail is the exception.\n",
    "2. Presumption of innocence applies until conviction.\n",
    "3. Consider BOTH aggravating and mitigating factors.\n",
    "\n",
    "Aggravating factors may include:\n",
    "- Strong prima facie evidence\n",
    "- Risk of absconding\n",
    "- Threat to witnesses\n",
    "- Repeat or organized criminal activity\n",
    "- Seriousness of offence affecting public order\n",
    "\n",
    "Mitigating factors may include:\n",
    "- No criminal antecedents\n",
    "- Judicial custody already undergone\n",
    "- Weak, general, or uncorroborated allegations\n",
    "- Offence not punishable with death or life imprisonment\n",
    "- No risk of tampering or absconding shown\n",
    "\n",
    "Decision rule:\n",
    "- Grant bail if mitigating factors outweigh aggravating factors.\n",
    "- Deny bail only if custody is necessary for justice or public safety.\n",
    "\n",
    "\n",
    "Respond with ONLY:\n",
    "\"yes\" → bail should be granted\n",
    "\"no\" → bail should be denied\n",
    "\"\"\"\n",
    "\n",
    "          conversation = [\n",
    "                    {\n",
    "                      \"role\": \"user\",\n",
    "                      \"content\": [\n",
    "                          {\"type\": \"image\", \"image\": image},\n",
    "                          {\"type\": \"text\", \"text\": prompt},\n",
    "                        ],\n",
    "                    }]\n",
    "          text = processor.apply_chat_template(\n",
    "                conversation, tokenize=False, add_generation_prompt=True\n",
    "          )\n",
    "          image_inputs, video_inputs = process_vision_info(conversation)\n",
    "          inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "           )\n",
    "          inputs = inputs.to(\"cuda\")\n",
    "\n",
    "          nb_text_tokens = inputs['input_ids'].shape[1]\n",
    "          patch_size = 14\n",
    "\n",
    "          generated_ids = model.generate(**inputs,return_dict_in_generate=True,\n",
    "                                         output_scores=True,\n",
    "                                         do_sample=True,\n",
    "                                         max_new_tokens=2,\n",
    "                                         temperature=0.7)\n",
    "\n",
    "\n",
    "          # Get all possible token IDs for yes/no\n",
    "          yes_tokens = []\n",
    "          no_tokens = []\n",
    "\n",
    "          for variant in [\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\", \"\\nyes\", \"\\nYes\"]:\n",
    "               token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "               if token_ids:\n",
    "                  yes_tokens.append(token_ids[0])\n",
    "\n",
    "          for variant in [\"no\" ,\" no\", \"No\", \" No\", \"NO\", \" NO\",\"\\nno\", \"\\nNo\"]:\n",
    "               token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "               if token_ids:\n",
    "                  no_tokens.append(token_ids[0])\n",
    "\n",
    "          # Remove duplicates\n",
    "          yes_tokens = list(set(yes_tokens))\n",
    "          no_tokens = list(set(no_tokens))\n",
    "\n",
    "          print(f\"Yes token IDs: {yes_tokens}\")\n",
    "          print(f\"No token IDs: {no_tokens}\")\n",
    "\n",
    "          # Now get probabilities for ALL of them\n",
    "          answer_probs = torch.softmax(generated_ids.scores[0][0], dim=0)\n",
    "\n",
    "          yes_prob = sum([answer_probs[tid].item() for tid in yes_tokens])\n",
    "          no_prob = sum([answer_probs[tid].item() for tid in no_tokens])\n",
    "\n",
    "          print(f\"Total Yes probability: {yes_prob:.4f}\")\n",
    "          print(f\"Total No probability: {no_prob:.4f}\")\n",
    "\n",
    "          h,w = 224,224\n",
    "          h_cols = math.ceil(h/14)\n",
    "          w_cols = math.ceil(w/14)\n",
    "          image_token = torch.tensor(range(1,h_cols*w_cols +1)).unsqueeze(0)\n",
    "          X = torch.cat( (inputs[\"input_ids\"].cuda(), image_token.cuda()), 1).unsqueeze(1)\n",
    "          explainer = shap.Explainer(\n",
    "                    get_model_prediction, custom_masker, silent=True,max_evals=4000)\n",
    "          shap_values = explainer(X.cpu())\n",
    "          mm_score = compute_mm_score(nb_text_tokens, shap_values)\n",
    "          results[\"mmscore\"].append(mm_score)\n",
    "          print(mm_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6690831031182322\n",
      "0.0825926557948698\n"
     ]
    }
   ],
   "source": [
    "means = np.array(results[\"mmscore\"]).mean()\n",
    "dev = np.array(results[\"mmscore\"]).std()\n",
    "print(means)\n",
    "print(dev)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
