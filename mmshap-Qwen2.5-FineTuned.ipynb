{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MM-SHAP' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Heidelberg-NLP/MM-SHAP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate shap (rampage)\n",
    "import shap\n",
    "import torch\n",
    "import sklearn\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, copy, json\n",
    "import re, math, sys\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "sys.path.insert(0, 'MM-SHAP') # Add MM-SHAP directory to Python path\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, CLIPImageProcessor,TorchAoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             id  \\\n",
      "0      Bail Application_2180_202002-01-20211157   \n",
      "1       Bail Application_1017_202006-07-2020391   \n",
      "2      Bail Application_1156_202122-02-20215574   \n",
      "3     Bail Application_101049_202131-03-2021293   \n",
      "4      Bail Application_4458_202006-10-20202515   \n",
      "5      Bail Application_3257_202031-10-20201419   \n",
      "6       Bail Application_1568_202004-06-2020954   \n",
      "7        Bail Application_939_201908-07-2019834   \n",
      "8        Bail Application__70_202117-02-2021393   \n",
      "9        Bail Application_5009_201929-08-201947   \n",
      "10     Bail Application_2394_201903-01-20201212   \n",
      "11     Bail Application_2018_202017-09-20207333   \n",
      "12     Bail Application_5624_201911-09-20193042   \n",
      "13      Bail Application_1027_202024-11-2020221   \n",
      "14   Bail Application_102278_202011-09-20202077   \n",
      "15        Bail Application_2820_202122-04-20217   \n",
      "16      Bail Application_804_202009-06-20201268   \n",
      "17      Bail Application_1438_202007-10-2020901   \n",
      "18         Bail Application_301_202021-03-20207   \n",
      "19       Bail Application_320_202105-03-2021907   \n",
      "20      Bail Application_160_202128-01-20211831   \n",
      "21      Bail Application_1354_202009-11-2020545   \n",
      "22     Bail Application_2182_202105-04-20213777   \n",
      "23       Bail Application_221_202006-06-2020119   \n",
      "24       Bail Application_168_202030-01-2020548   \n",
      "25    Bail_Application_100488_201901-10-2019521   \n",
      "26     Bail Application_4567_202030-09-20202700   \n",
      "27  Bail Application_3000275_201924-10-20199807   \n",
      "28       Bail Application_943_202026-02-2020480   \n",
      "29       Bail Application_1149_202109-04-202147   \n",
      "30              Bail_3300847_201907-09-20195701   \n",
      "31       Bail Application_923_201906-07-2019742   \n",
      "32      Bail Application_2873_202027-07-2020628   \n",
      "33     Bail Application_1030_202119-03-20212203   \n",
      "34     Bail Application_1381_202007-01-20211107   \n",
      "35        Bail Application_518_202106-04-202197   \n",
      "36     Bail Application_1607_202109-03-20212028   \n",
      "37  Bail Application_2102142_201914-11-20199326   \n",
      "38     Bail Application_5606_202031-10-20201718   \n",
      "39       Bail Application_304_202116-03-2021259   \n",
      "40     Bail Application_2955_201910-10-20191660   \n",
      "41     Bail Application_1096_202028-05-20201153   \n",
      "42       Bail Application_140_202029-02-2020150   \n",
      "43     Bail Application_2270_201919-09-20191796   \n",
      "44      Bail Application_1476_202014-10-2020639   \n",
      "45      Bail Application_1211_201927-06-2019626   \n",
      "46     Bail Application_2137_202026-06-20201529   \n",
      "47       Bail Application_517_201930-10-2019108   \n",
      "48       Bail Application_719_202008-06-2020999   \n",
      "49           Bail Appn__2036_201917-07-20191021   \n",
      "\n",
      "                                  facts_and_arguments  label  \n",
      "0   When the plaintiff Kibahan told the above thin...      0  \n",
      "1   According to the prosecution, the inspector-in...      1  \n",
      "2   The accused is in judicial custody. The learne...      1  \n",
      "3   The investigator has compiled sufficient again...      1  \n",
      "4   According to the story, the plaintiff, Guru, f...      1  \n",
      "5   In support of the bail application, the applic...      1  \n",
      "6   The grounds of bail on behalf of the applicant...      1  \n",
      "7   Briefly, according to the First Information Re...      1  \n",
      "8   The accused are lodged in the district jail wi...      0  \n",
      "9   The bail application states that the applicant...      1  \n",
      "10  The affidavit submitted by < name > < name > s...      0  \n",
      "11  The accused is lodged in jail under judicial c...      0  \n",
      "12  The accused is in judicial custody. In a nutsh...      1  \n",
      "13  In brief, the facts of the case are as follows...      0  \n",
      "14  The accused is under detention in District Jai...      1  \n",
      "15  An affidavit in support of the bail applicatio...      0  \n",
      "16  3-According to the brief version of the presen...      0  \n",
      "17  In short, the story is that the plaintiff, Suc...      0  \n",
      "18  The bail application has been moved by the app...      1  \n",
      "19  Affidavit < name > has been submitted along wi...      1  \n",
      "20  2. In a nutshell, the story is as follows: The...      1  \n",
      "21  2. This is the first bail application of the a...      1  \n",
      "22  The accused have surrendered before the and ar...      1  \n",
      "23  2- In short, the plot is as follows: The plain...      1  \n",
      "24  Briefly, the plot is as under: Date 19.01.2020...      1  \n",
      "25  T & 3 (1) Dt. Jati & Anujjati (Prevention of A...      1  \n",
      "26  The learned arguments of the learned counsel f...      0  \n",
      "27  2. In brief, the states that the plaintiff एस....      1  \n",
      "28  The accused is in judicial since the date 06.0...      1  \n",
      "29  Briefly, the story is as follows: A First Info...      1  \n",
      "30  Date: - 07.09.2019अभियुक्त The bail applicatio...      0  \n",
      "31  A bail application has been submitted on behal...      1  \n",
      "32  The 0-27-07 -2020 order today led to the appea...      1  \n",
      "33  The applicant / accused is under judicial in t...      0  \n",
      "34  2- In short, according to the story, the date ...      0  \n",
      "35  The learned counsel for the applicant submits ...      0  \n",
      "36  In support of the bail application, an affidav...      0  \n",
      "37  The accused is in the district jail under judi...      1  \n",
      "38  An affidavit in support of the bail applicatio...      1  \n",
      "39  The case of the is that on the date 05.03.2021...      0  \n",
      "40  The bail application by the applicant / accuse...      1  \n",
      "41  The candidate is in judicial in the above case...      1  \n",
      "42  The learned counsel for the plaintiff / accuse...      0  \n",
      "43  Briefly, the of the is as follows: The plainti...      1  \n",
      "44  The bail application and affidavit submitted o...      1  \n",
      "45  From the perusal of the First Information Repo...      1  \n",
      "46  The affidavit along with the application was s...      1  \n",
      "47  The accused is in judicial custody. The bail a...      1  \n",
      "48  I listened to the arguments of the learned cou...      1  \n",
      "49  The bail application of the candidate has been...      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_preprocessed.csv', on_bad_lines='skip')\n",
    "df = df.iloc[:50]\n",
    "df = df[['id','facts_and_arguments','label']]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.0)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c560c896da4be4be1b8f8c5d6b318d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0f0805f9224ce29520c18ebc0877f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d56cee61a5425c937cf59052a597fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded2eba6a23e4b8ca76443f1d6ec6ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787d6c2475284ba6ae7a500f65971b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d80d2871ac4b0f8d44b6119e5e970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde24574779a414c847f18b7bb5b4377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_token': '<|image_pad|>', 'video_token': '<|video_pad|>', 'image_token_id': 151655, 'video_token_id': 151656, 'chat_template': \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\", 'audio_tokenizer': None, 'image_processor': Qwen2VLImageProcessorFast {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"disable_grouping\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_pixels\": 1003520,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 200704,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_tensors\": null,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      ", 'tokenizer': Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      "), 'video_processor': Qwen2VLVideoProcessor {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"do_sample_frames\": false,\n",
      "  \"fps\": null,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_frames\": 768,\n",
      "  \"max_pixels\": 1003520,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_frames\": 4,\n",
      "  \"min_pixels\": 200704,\n",
      "  \"num_frames\": null,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_metadata\": false,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2,\n",
      "  \"video_metadata\": null,\n",
      "  \"video_processor_type\": \"Qwen2VLVideoProcessor\"\n",
      "}\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "            (proj): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "            (up_proj): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "            (down_proj): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear4bit(in_features=5120, out_features=3584, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(152064, 3584)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "num_samples = \"all\"\n",
    "if num_samples != \"all\":\n",
    "    num_samples = int(num_samples)\n",
    "\n",
    "write_res = \"yes\"\n",
    "task = \"image_sentence_alignment\"\n",
    "other_tasks_than_valse = ['mscoco','vqa','gqa','gqa_balanced','nlvr2']\n",
    "use_cuda = True\n",
    "\n",
    "model_name = \"/content/drive/MyDrive/qwen2.5vl-7b-instruct-sft-freeze-visual-image-masked\"\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "min_pixels = 256 * 28 * 28\n",
    "max_pixels = 1280 * 28 * 28\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "   model_id , min_pixels=min_pixels, max_pixels=max_pixels\n",
    ")\n",
    "print(processor.__dict__ )\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_masker(mask, x):\n",
    "    \"\"\"\n",
    "    Shap relevant function.\n",
    "    It gets a mask from the shap library with truth values about which image and text tokens to mask (False) and which not (True).\n",
    "    It defines how to mask the text tokens and masks the text tokens. So far, we don't mask the image, but have only defined which image tokens to mask. The image tokens masking happens in get_model_prediction().\n",
    "    \"\"\"\n",
    "    masked_X = x.clone()\n",
    "    mask = torch.tensor(mask).unsqueeze(0)\n",
    "    masked_X[~mask] = 0  # ~mask !!! to zero\n",
    "    for i in range(masked_X.shape[0]):\n",
    "      for j in range(masked_X.shape[1]):\n",
    "       if x[i,j] >= 151643 and x[i,j]<=151664 :\n",
    "         masked_X[i,j]=x[i,j]\n",
    "    return masked_X\n",
    "def compute_mm_score(text_length, shap_values):\n",
    "    \"\"\" Compute Multimodality Score. (80% textual, 20% visual, possibly: 0% knowledge). \"\"\"\n",
    "    sum=0\n",
    "    for i in range(text_length):\n",
    "      if inputs[\"input_ids\"][0][i] >= 151643 and inputs[\"input_ids\"][0][i]<=151664 :\n",
    "        continue\n",
    "      else:\n",
    "        sum = sum + np.abs(shap_values.values[0, 0, i])\n",
    "    text_contrib = sum\n",
    "    image_contrib = np.abs(shap_values.values[0, 0, text_length:]).sum()\n",
    "    text_score = text_contrib / (text_contrib + image_contrib)\n",
    "    # image_score = image_contrib / (text_contrib + image_contrib) # is just 1 - text_score in the two modalities case\n",
    "    return text_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(x):\n",
    "    \"\"\"\n",
    "    Shap relevant function.\n",
    "    1. Mask the image pixel according to the specified patches to mask from the custom masker.\n",
    "    2. Predict the model output for all combinations of masked image and tokens. This is then further passed to the shap libary.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(x[:, :inputs[\"input_ids\"].shape[1]])\n",
    "        masked_image_token_ids = torch.tensor(x[:, inputs[\"input_ids\"].shape[1]:])\n",
    "        if use_cuda:\n",
    "            input_ids = input_ids.cuda()\n",
    "            masked_image_token_ids = masked_image_token_ids.cuda()\n",
    "        result = np.zeros(input_ids.shape[0])\n",
    "        row_cols = 16\n",
    "        patch_size=14\n",
    "        for i in range(inputs['input_ids'].shape[0]):\n",
    "          masked_text_inputs = inputs.copy()\n",
    "          masked_text_inputs['input_ids'] = input_ids[i].unsqueeze(0)\n",
    "          masked_image = np.array(image)\n",
    "          for k in range(masked_image_token_ids[i].shape[0]):\n",
    "            if masked_image_token_ids[i][k] == 0:  # should be zero\n",
    "                    m = k // row_cols  # 384 (img shape) / 16 (patch size)\n",
    "                    n = k % row_cols\n",
    "                    masked_image[ m * patch_size:(m+1)*patch_size, n*patch_size:(\n",
    "                        n+1)*patch_size,:] = 0\n",
    "          imaged = Image.fromarray(masked_image)\n",
    "          modified_text_clean = processor.tokenizer.decode( masked_text_inputs['input_ids'][0].cuda(), skip_special_tokens=True)\n",
    "          messages = [\n",
    "              {\n",
    "                 \"role\": \"user\",\n",
    "                 \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": imaged},\n",
    "                    {\"type\": \"text\", \"text\": modified_text_clean}\n",
    "                    ]}]\n",
    "          text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "          inputd = processor(\n",
    "              text=text,images=imaged,return_tensors=\"pt\",padding=True).to(\"cuda\")\n",
    "          outputs = model.generate(**inputd, return_dict_in_generate=True, output_scores=True,do_sample=False,       # Greedy decoding\n",
    "               max_new_tokens=1,temperature=0.1)\n",
    "          yes_tokens = []\n",
    "          no_tokens = []\n",
    "\n",
    "          for variant in [\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\"]:\n",
    "                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "                if token_ids:\n",
    "                   yes_tokens.append(token_ids[0])\n",
    "\n",
    "          for variant in [\"no\", \" no\", \"No\", \" No\", \"NO\", \" NO\"]:\n",
    "                token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "                if token_ids:\n",
    "                   no_tokens.append(token_ids[0])\n",
    "\n",
    "\n",
    "          yes_tokens = list(set(yes_tokens))\n",
    "          no_tokens = list(set(no_tokens))\n",
    "          answer_probs = torch.softmax(outputs.scores[0][0], dim=0)\n",
    "          yes_prob = sum([answer_probs[tid].item() for tid in yes_tokens])\n",
    "          no_prob = sum([answer_probs[tid].item() for tid in no_tokens])\n",
    "          if label==1:\n",
    "            result[i] = yes_prob\n",
    "          else:\n",
    "            result[i] = no_prob\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qwen_vl_utils\n",
      "  Downloading qwen_vl_utils-0.0.14-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting av (from qwen_vl_utils)\n",
      "  Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (25.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (11.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (2025.11.12)\n",
      "Downloading qwen_vl_utils-0.0.14-py3-none-any.whl (8.1 kB)\n",
      "Downloading av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: av, qwen_vl_utils\n",
      "Successfully installed av-16.0.1 qwen_vl_utils-0.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install qwen_vl_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1449\n",
      "Total No probability: 0.8549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7322016294169871\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0270\n",
      "Total No probability: 0.9729\n",
      "0.6591966560842483\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.6154\n",
      "Total No probability: 0.3845\n",
      "0.5451114523130195\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0002\n",
      "Total No probability: 0.9998\n",
      "0.4878575961885892\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2196\n",
      "Total No probability: 0.7802\n",
      "0.8035579343306236\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1392\n",
      "Total No probability: 0.8607\n",
      "0.7730258214087126\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1735\n",
      "Total No probability: 0.8261\n",
      "0.6498130413789559\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0389\n",
      "Total No probability: 0.9590\n",
      "0.6302080783093034\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0490\n",
      "Total No probability: 0.9477\n",
      "0.5432135610166772\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.6453\n",
      "Total No probability: 0.3547\n",
      "0.7013594548075516\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2968\n",
      "Total No probability: 0.7030\n",
      "0.5636722117771151\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.3523\n",
      "Total No probability: 0.6470\n",
      "0.48020152987820625\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0660\n",
      "Total No probability: 0.9339\n",
      "0.7984423619906099\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1223\n",
      "Total No probability: 0.8776\n",
      "0.5454333112640489\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0345\n",
      "Total No probability: 0.9655\n",
      "0.683647941131357\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.4699\n",
      "Total No probability: 0.5299\n",
      "0.710348312853725\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0742\n",
      "Total No probability: 0.9257\n",
      "0.7925932003650864\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0211\n",
      "Total No probability: 0.9788\n",
      "0.7118213091432865\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0129\n",
      "Total No probability: 0.9866\n",
      "0.6266456703281873\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2118\n",
      "Total No probability: 0.7882\n",
      "0.5340957588251286\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.3049\n",
      "Total No probability: 0.6949\n",
      "0.7433330864981732\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2159\n",
      "Total No probability: 0.7840\n",
      "0.7565167732013816\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.6882\n",
      "Total No probability: 0.3116\n",
      "0.5971384939145911\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.3183\n",
      "Total No probability: 0.6816\n",
      "0.808864681168935\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1756\n",
      "Total No probability: 0.8243\n",
      "0.8385971027904782\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0834\n",
      "Total No probability: 0.9165\n",
      "0.592818416019847\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2208\n",
      "Total No probability: 0.7790\n",
      "0.786268615377072\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0945\n",
      "Total No probability: 0.9053\n",
      "0.733640862939419\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1292\n",
      "Total No probability: 0.8708\n",
      "0.7358998781330137\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.4015\n",
      "Total No probability: 0.5983\n",
      "0.5828681512984667\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1158\n",
      "Total No probability: 0.8821\n",
      "0.5590054123491087\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.5588\n",
      "Total No probability: 0.4411\n",
      "0.7270984908873092\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.4175\n",
      "Total No probability: 0.5824\n",
      "0.6845685684491138\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.5701\n",
      "Total No probability: 0.4296\n",
      "0.5686502303498224\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0275\n",
      "Total No probability: 0.9724\n",
      "0.7176927875959833\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2966\n",
      "Total No probability: 0.7032\n",
      "0.8176749955496936\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1727\n",
      "Total No probability: 0.8272\n",
      "0.673022594325069\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1528\n",
      "Total No probability: 0.8469\n",
      "0.7234386799102164\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.3323\n",
      "Total No probability: 0.6675\n",
      "0.6387252347286315\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2081\n",
      "Total No probability: 0.7918\n",
      "0.7359080639826721\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.6600\n",
      "Total No probability: 0.3400\n",
      "0.5293698188744387\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1029\n",
      "Total No probability: 0.8970\n",
      "0.6690273745327654\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.5902\n",
      "Total No probability: 0.4097\n",
      "0.6784260808084801\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.0976\n",
      "Total No probability: 0.9021\n",
      "0.6634369292855747\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.1243\n",
      "Total No probability: 0.8756\n",
      "0.5315588375560771\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.7228\n",
      "Total No probability: 0.2771\n",
      "0.6574755889630346\n",
      "Yes token IDs: [14080, 198, 9834, 9454, 14004, 7414, 9693]\n",
      "No token IDs: [5664, 2753, 2308, 8996, 902, 198, 2152]\n",
      "Total Yes probability: 0.2660\n",
      "Total No probability: 0.7339\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3353993996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m           explainer = shap.Explainer(\n\u001b[1;32m    122\u001b[0m                     get_model_prediction, custom_masker, silent=True,max_evals=4000)\n\u001b[0;32m--> 123\u001b[0;31m           \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m           \u001b[0mmm_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_mm_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_text_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshap_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m           \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mmscore\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmm_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 ):\n\u001b[0;32m---> 73\u001b[0;31m                     return super().__call__(\n\u001b[0m\u001b[1;32m     74\u001b[0m                         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                         \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     98\u001b[0m     ):\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         return super().__call__(\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow_args\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" explainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             row_result = self.explain_row(\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mrow_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36mexplain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# evaluate the masked model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrow_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mfull_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_masker_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0m_convert_delta_mask_to_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_masking_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/utils/_masked_model.py\u001b[0m in \u001b[0;36m_full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mjoined_masked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_masked_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjoined_masked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0m_assert_output_input_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_masked_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mall_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/shap/models/_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mis_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"torch.Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tensor\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1609711077.py\u001b[0m in \u001b[0;36mget_model_prediction\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     36\u001b[0m           inputd = processor(\n\u001b[1;32m     37\u001b[0m               text=text,images=imaged,return_tensors=\"pt\",padding=True).to(\"cuda\")\n\u001b[0;32m---> 38\u001b[0;31m           outputs = model.generate(**inputd, return_dict_in_generate=True, output_scores=True,do_sample=False,       # Greedy decoding\n\u001b[0m\u001b[1;32m     39\u001b[0m                max_new_tokens=1,temperature=0.1)\n\u001b[1;32m     40\u001b[0m           \u001b[0myes_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         )\n\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1477\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, rope_deltas, cache_position, second_per_grid_ts, **kwargs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m             \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_grid_thw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m             \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             image_mask, _ = self.get_placeholder_mask(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mget_image_features\u001b[0;34m(self, pixel_values, image_grid_thw)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \"\"\"\n\u001b[1;32m   1169\u001b[0m         \u001b[0mpixel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_thw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_grid_thw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m         \u001b[0msplit_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_grid_thw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_merge_size\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, grid_thw, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m         \"\"\"\n\u001b[1;32m    417\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mrotary_pos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_thw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mwindow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcu_window_seqlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_window_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_thw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         cu_window_seqlens = torch.tensor(\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\u001b[0m in \u001b[0;36mrot_pos_emb\u001b[0;34m(self, grid_thw)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mpos_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_thw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mhpos_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             hpos_ids = hpos_ids.reshape(\n\u001b[1;32m    341\u001b[0m                 \u001b[0mh\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_merge_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from IPython.display import display\n",
    "results = {\n",
    "        'mmscore': [],\n",
    "}\n",
    "for i in range(len(df)):\n",
    "\n",
    "    test_sentences = [df.iloc[i]['facts_and_arguments']]\n",
    "    label = df.iloc[i]['label']\n",
    "    test_img_path = f\"/content/image{i+1}.jpg\"\n",
    "    image = Image.open(test_img_path)\n",
    "    image = image.resize((224, 224), Image.BICUBIC)\n",
    "    image = image.convert(\"RGB\")\n",
    "\n",
    "\n",
    "    for k,sentence in enumerate(test_sentences):\n",
    "          prompt = f\"\"\"\n",
    "You are an impartial criminal law judge deciding a bail application.\n",
    "\n",
    "Based ONLY on the facts provided , decide whether bail should be granted.\n",
    "\n",
    "Evidence description:\n",
    "\"{sentence}\"\n",
    "You MUST apply the following principles:\n",
    "1. Bail is the rule; jail is the exception.\n",
    "2. Presumption of innocence applies until conviction.\n",
    "3. Consider BOTH aggravating and mitigating factors.\n",
    "\n",
    "Aggravating factors may include:\n",
    "- Strong prima facie evidence\n",
    "- Risk of absconding\n",
    "- Threat to witnesses\n",
    "- Repeat or organized criminal activity\n",
    "- Seriousness of offence affecting public order\n",
    "\n",
    "Mitigating factors may include:\n",
    "- No criminal antecedents\n",
    "- Judicial custody already undergone\n",
    "- Weak, general, or uncorroborated allegations\n",
    "- Offence not punishable with death or life imprisonment\n",
    "- No risk of tampering or absconding shown\n",
    "\n",
    "Decision rule:\n",
    "- Grant bail if mitigating factors outweigh aggravating factors.\n",
    "- Deny bail only if custody is necessary for justice or public safety.\n",
    "\n",
    "\n",
    "Respond with ONLY:\n",
    "\"yes\" → bail should be granted\n",
    "\"no\" → bail should be denied\n",
    "\"\"\"\n",
    "\n",
    "          conversation = [\n",
    "                    {\n",
    "                      \"role\": \"user\",\n",
    "                      \"content\": [\n",
    "                          {\"type\": \"image\", \"image\": image},\n",
    "                          {\"type\": \"text\", \"text\": prompt},\n",
    "                        ],\n",
    "                    }]\n",
    "          text = processor.apply_chat_template(\n",
    "                conversation, tokenize=False, add_generation_prompt=True\n",
    "          )\n",
    "          image_inputs, video_inputs = process_vision_info(conversation)\n",
    "          inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "           )\n",
    "          inputs = inputs.to(\"cuda\")\n",
    "\n",
    "          nb_text_tokens = inputs['input_ids'].shape[1]\n",
    "          patch_size = 14\n",
    "\n",
    "          generated_ids = model.generate(**inputs,return_dict_in_generate=True,\n",
    "                                         output_scores=True,\n",
    "                                         do_sample=True,\n",
    "                                         max_new_tokens=2,\n",
    "                                         temperature=0.7)\n",
    "\n",
    "\n",
    "          # Get all possible token IDs for yes/no\n",
    "          yes_tokens = []\n",
    "          no_tokens = []\n",
    "\n",
    "          for variant in [\"yes\", \" yes\", \"Yes\", \" Yes\", \"YES\", \" YES\", \"\\nyes\", \"\\nYes\"]:\n",
    "               token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "               if token_ids:\n",
    "                  yes_tokens.append(token_ids[0])\n",
    "\n",
    "          for variant in [\"no\" ,\" no\", \"No\", \" No\", \"NO\", \" NO\",\"\\nno\", \"\\nNo\"]:\n",
    "               token_ids = processor.tokenizer.encode(variant, add_special_tokens=False)\n",
    "               if token_ids:\n",
    "                  no_tokens.append(token_ids[0])\n",
    "\n",
    "          # Remove duplicates\n",
    "          yes_tokens = list(set(yes_tokens))\n",
    "          no_tokens = list(set(no_tokens))\n",
    "\n",
    "          print(f\"Yes token IDs: {yes_tokens}\")\n",
    "          print(f\"No token IDs: {no_tokens}\")\n",
    "\n",
    "          # Now get probabilities for ALL of them\n",
    "          answer_probs = torch.softmax(generated_ids.scores[0][0], dim=0)\n",
    "\n",
    "          yes_prob = sum([answer_probs[tid].item() for tid in yes_tokens])\n",
    "          no_prob = sum([answer_probs[tid].item() for tid in no_tokens])\n",
    "\n",
    "          print(f\"Total Yes probability: {yes_prob:.4f}\")\n",
    "          print(f\"Total No probability: {no_prob:.4f}\")\n",
    "\n",
    "          h,w = 224,224\n",
    "          h_cols = math.ceil(h/14)\n",
    "          w_cols = math.ceil(w/14)\n",
    "          image_token = torch.tensor(range(1,h_cols*w_cols +1)).unsqueeze(0)\n",
    "          X = torch.cat( (inputs[\"input_ids\"].cuda(), image_token.cuda()), 1).unsqueeze(1)\n",
    "          explainer = shap.Explainer(\n",
    "                    get_model_prediction, custom_masker, silent=True,max_evals=4000)\n",
    "          shap_values = explainer(X.cpu())\n",
    "          mm_score = compute_mm_score(nb_text_tokens, shap_values)\n",
    "          results[\"mmscore\"].append(mm_score)\n",
    "          print(mm_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.667901577876104\n",
      "0.0952378134849873\n"
     ]
    }
   ],
   "source": [
    "means = np.array(results[\"mmscore\"]).mean()\n",
    "dev = np.array(results[\"mmscore\"]).std()\n",
    "print(means)\n",
    "print(dev)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
